{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-bnrm_jRjBW",
        "outputId": "41ac7c60-1ac7-491e-c2ce-39e40ef10a7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 154 kB 4.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 352 kB 8.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 85 kB 1.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip install -q optax dm-haiku"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import haiku as hk\n"
      ],
      "metadata": {
        "id": "7no4en8ORmN3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solving for the optimal consumption policy and generating 1 million sample paths of expected sum of discounted rewards (value function) resulting from that policy, assuming savings are fully invested in the stock market, so the evolution of wealth is now stochastic."
      ],
      "metadata": {
        "id": "8jkufv5SSAMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "γ = 2.\n",
        "β = 0.95\n",
        "\n",
        "def stock_return(rng):\n",
        "  μs = 0.06\n",
        "  σs = 0.2\n",
        "  ε = jax.random.normal(rng, ())\n",
        "  log_return = μs + σs * ε\n",
        "  return jnp.exp(log_return)\n",
        "\n",
        "def U(c):\n",
        "    return c**(1 - γ) / (1 - γ)\n",
        "\n",
        "\n",
        "optimizer = optax.adam\n",
        "lr = 1e-3\n",
        "T = 50\n",
        "\n",
        "\n",
        "def nnet(x):\n",
        "  X = jnp.column_stack([x])\n",
        "  X = hk.Linear(32)(X)\n",
        "  X = jnp.tanh(X)\n",
        "  X = hk.Linear(1)(X)\n",
        "  X = jnp.squeeze(X)\n",
        "  return X\n",
        "\n",
        "\n",
        "init, nnet = hk.without_apply_rng(hk.transform(nnet))\n",
        "rng = jax.random.PRNGKey(0)\n",
        "Θ = init(rng, jnp.array(1.))\n",
        "\n",
        "opt_state = optimizer(lr).init(Θ)\n",
        "\n",
        "def L(Θ, rng):\n",
        "\n",
        "  x = 1.\n",
        "  G = 0.\n",
        "\n",
        "  state = x\n",
        "  inputs = jnp.arange(T)\n",
        "##########################################################\n",
        "\n",
        "# creating a vector of returns for different time\n",
        "\n",
        "  def ret(rng, inputs):\n",
        "    rng, _ = jax.random.split(rng)\n",
        "    R = stock_return(rng)\n",
        "    return rng, R\n",
        "  rng, R = jax.lax.scan(ret, rng, inputs) \n",
        "##########################################################\n",
        "    \n",
        "  def core(state, inputs):\n",
        "    t = inputs\n",
        "    xt = state\n",
        "    r = R[inputs-1]# selecting a randomly generated return \n",
        "    ct = jax.nn.sigmoid(nnet(Θ, xt) - 4.) * xt\n",
        "    ut = U(ct)\n",
        "    savings = xt - ct\n",
        "    x_tp1 = (r) * savings # generating new state with savings growing at rate r\n",
        "\n",
        "    discounted_utility = β**t * ut\n",
        "    return x_tp1, discounted_utility\n",
        "\n",
        "  x, discounted_utility = jax.lax.scan(core, state, inputs)\n",
        "  G = discounted_utility.sum()\n",
        "  return -G\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def evaluation(Θ, rng):\n",
        "  return -L(Θ, rng)\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def update_gradient_descent(Θ, opt_state, rng):\n",
        "  rng, _ = jax.random.split(rng)\n",
        "  grad = jax.grad(L)(Θ,rng)\n",
        "  updates, opt_state = optimizer(lr).update(grad, opt_state)\n",
        "  Θ = optax.apply_updates(Θ, updates)\n",
        "  return Θ, opt_state, rng\n",
        "\n",
        "\n",
        "for iteration in range(1000000):\n",
        "  rng = jax.random.PRNGKey(0)\n",
        "  Θ, opt_state, rng = update_gradient_descent(Θ, opt_state, rng)\n",
        "  \n",
        "\n",
        "  if iteration % 1000 == 0:\n",
        "    print(evaluation(Θ, rng))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKi0JZGRRpir",
        "outputId": "f32122ad-c17a-49db-d409-c0f6b7057014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-768.27893\n",
            "-443.21417\n",
            "-442.118\n",
            "-439.13965\n",
            "-437.8162\n",
            "-437.30887\n",
            "-437.05707\n",
            "-436.6582\n",
            "-436.6275\n",
            "-436.28903\n"
          ]
        }
      ]
    }
  ]
}